{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.modeling_diffbert_sample import DiffBertForDiffusion\n",
    "from src.modeling_diffllama import DiffLlamaForDiffusionLM\n",
    "from src.configuration_diffbert import DiffBertConfig\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/diffllama-mini-sample\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "scheduler = DDPMScheduler.from_pretrained(\"models/diffllama-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffLlamaForDiffusionLM.from_pretrained(\"models/diffllama-mini-sample-trained-prompts\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use a scheduler with more steps than we trained on (sometimes it gives even better results)\n",
    "scheduler = DDPMScheduler(prediction_type=\"sample\", num_train_timesteps=2500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def get_timesteps(num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[t_start * scheduler.order :]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "        \n",
    "def vectors_to_indices(vectors):\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    return indices\n",
    "\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL --->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0    --->    a5 of d,inive highlys in anime cyborg,beribrant  art k, cinematicematic lighting, aa focus, dramatic cinematic,e b of a beautiful, alphonse, highP,s detailed on, elegant'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1    --->    cinematicic poster of beautiful bgle wearing ornate, pixiv, 4 k, art by artgerm, by high, style full body body, realistic, artstation, trending on artstation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2    --->    3 wrealra-up portrait vamp portrait phot detailedalistic and fine- digital, art, smoothis digital detailed art, portrait,uristic illustredinski, dndoprealistic, cinematic lighting, fanticate intric high details, masterpiece'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3    --->    a art painting of a sharpraitsterim with dreamy and character with a  man, be detailed is and beautiful painting, concept art, gying a, art, artgerm, highly- pretty face, volumeting trending on Artstation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4    --->    j ofin man, the is on background, by greggarte, concept art, mat detailed, short hair,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5    --->    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'6    --->    a realistic art portraitant. art award. cute and background and sh detailed, sharp focus, trending on artstation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7    --->    a art by as as gilate fulled body portraitsi is portrait, highate face, lighting, moody, intricate w wu, preal mohr, trending on artstation, digital art, detailed, sharp focus, illustration, intricate, cinematic,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((8, 64, 4096), device=device).to(torch.float16)\n",
    "    attention_mask = torch.ones((8, 64), device=device)\n",
    "    num_inference_steps = 2000\n",
    "    timesteps=None\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        rnd_latents = torch.rand((1, 64, 4096), device=device).to(torch.float16)\n",
    "\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).long().to(device),\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        latents_final = outputs.logits\n",
    "        if i % 100 ==0 :\n",
    "            clear_output(wait=True)\n",
    "            display(f\"SAMPLES[{i}]--->\")\n",
    "            for n in range(latents_final.shape[0]):\n",
    "                display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "            display(\"---------------\")\n",
    "\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"FINAL --->\")\n",
    "for n in range(latents_final.shape[0]):\n",
    "    display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "display(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
