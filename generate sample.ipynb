{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDPMScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from modeling_diffbert_sample import DiffBertForDiffusion\n",
    "from configuration_diffbert import DiffBertConfig\n",
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "scheduler = DDPMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffBertForDiffusion.from_pretrained(\"diffbert-mini-sample-trained\").to(\"cuda\")\n",
    "device = model.device\n",
    "embedding = BertForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\").to(device)#torch.nn.Embedding(model.config.vocab_size, model.config.hidden_size).to(device)\n",
    "# embedding.load_state_dict(torch.load('diffbert-mini/embedding_weights.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def id_to_one_hot(token_ids, vocab_size=tokenizer.vocab_size):\n",
    "    one_hot_vectors = []\n",
    "    for token_id in token_ids:\n",
    "        # Create a zero-filled array with length equal to vocab_size\n",
    "        one_hot = torch.zeros(vocab_size)\n",
    "        # Set the value at the index of the token ID to 1\n",
    "        one_hot[token_id] = 1\n",
    "        one_hot_vectors.append(one_hot)\n",
    "    return torch.stack(one_hot_vectors, dim=0)\n",
    "\n",
    "def get_max_indices(list_of_tensors):\n",
    "    max_indices = []\n",
    "    for tensor in list_of_tensors:\n",
    "        # Get the index of the maximum value in the tensor\n",
    "        index = torch.argmax(tensor).item()\n",
    "        max_indices.append(index)\n",
    "    return max_indices\n",
    "# Function to transform vectors back to indices\n",
    "def vectors_to_indices(vectors, embedding):\n",
    "    # Calculate cosine similarity between vectors and all embedding weights\n",
    "    # similarity = torch.matmul(vectors, embedding.weight.T)\n",
    "    \n",
    "    # Get the index of the most similar embedding for each vector\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    \n",
    "    return indices\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8234, 0.9887, 0.5801,  ..., 0.3357, 0.0893, 0.1092],\n",
      "         [0.5783, 0.2146, 0.5057,  ..., 0.9016, 0.1550, 0.8159],\n",
      "         [0.8826, 0.3938, 0.5698,  ..., 0.6492, 0.9796, 0.7409],\n",
      "         ...,\n",
      "         [0.2288, 0.3776, 0.8710,  ..., 0.4842, 0.9594, 0.9722],\n",
      "         [0.3592, 0.2566, 0.0638,  ..., 0.7040, 0.1135, 0.1932],\n",
      "         [0.4192, 0.2948, 0.1341,  ..., 0.2115, 0.8594, 0.5207]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b05b06daf184afb945f1ec367d46f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((1, 128, 768), device=device)\n",
    "    print(latents)\n",
    "    num_inference_steps = 2000\n",
    "    timesteps=None#[999, 500, 1]\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "    # print(timesteps)\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        # predict the noise residual\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).to(device),\n",
    "            # encoder_hidden_states=prompt_embeds,\n",
    "            # timestep_cond=timestep_cond,\n",
    "            # cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "            # added_cond_kwargs=added_cond_kwargs,\n",
    "            # return_dict=False,\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        latents_final = outputs.logits\n",
    "\n",
    "\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        # print(scheduler.step(noise_pred, t, latents, return_dict=True))\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "        # latents_final = step[\"pred_original_sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0815, -7.2645, -7.6150,  ..., -7.6257, -6.7353, -7.3195],\n",
       "         [-1.3201, -8.1812, -9.5754,  ..., -8.7641, -8.3628, -9.2203],\n",
       "         [ 0.9122, -8.9232, -9.4601,  ..., -9.6429, -8.5498, -9.2966],\n",
       "         ...,\n",
       "         [-0.5197, -7.8803, -8.7028,  ..., -8.6381, -8.0927, -9.1499],\n",
       "         [-0.7567, -7.5069, -8.4107,  ..., -8.3093, -6.2122, -7.5812],\n",
       "         [ 8.9878, -6.1929, -6.8535,  ..., -6.9554, -6.0324, -6.8638]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.9424,  4.7998,  3.8280,  3.6118,  3.3611], device='cuda:0')\n",
      "tensor(101, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] from a -,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "top_k_values, top_k_indices = torch.topk(latents_final[0][0], k=5)\n",
    "print(top_k_values)\n",
    "print(top_k_indices[0])\n",
    "tokenizer.decode(top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 2043, 2014, 2057, 2038, 2001, 2044, 2012, 2016, 2040, 2001, 2016,\n",
      "        2001, 2012, 2001, 2012, 2001, 2012, 1037, 2038, 2016, 2013, 2016, 2001,\n",
      "        2038, 2042, 2048, 2016, 2001, 2016, 2038, 2042, 2016, 2042, 2001, 2042,\n",
      "        2012, 2043, 2012, 2013, 2016, 2420, 2012, 2038, 2016, 2001, 1037, 2042,\n",
      "        2025, 2016, 2016, 2001, 2042, 2014, 2016, 2001, 2043, 2042, 1011, 2016,\n",
      "        2042, 2012, 2038, 2042, 2042, 2001, 2016, 2014, 2012, 2001, 2042, 2012,\n",
      "        1017, 2043, 2016, 2012, 2037, 2001, 2016, 2074, 2024, 2016, 2012, 2014,\n",
      "        2001, 2016, 2001, 2042, 2043, 2042, 2013, 2138, 2042, 1011, 2013, 1037,\n",
      "        2038, 2042, 2001, 2016, 2001, 2001, 2001, 1997, 1037, 2014, 2016, 2096,\n",
      "        2016, 2016, 2012, 2035, 2038, 2042, 2042, 2012, 2016, 1011, 2014, 2482,\n",
      "        2072, 2001, 2042, 2012, 2001, 2038, 2014,  102], device='cuda:0')\n",
      "[CLS] when we we has was after at she who was she was at was at was at a has she from she was has been two she was she has been she been was been at when at from shebid at has she was a been hard she she was been her she was when been - she been at has been been was she her at was been at 3 we she at their was she just are she at her was she was 000 when been from, been - from a has been was she was was was of a her she 5 she she at queensland has been been at she - her ranch gr was been at was has her [SEP]\n",
      "[CLS] when her we has was after at she who was she was at was at was at a has she from she was has been two she was she has been she been was been at when at from she days at has she was a been not she she was been her she was when been - she been at has been been was she her at was been at 3 when she at their was she just are she at her was she was been when been from because been - from a has been was she was was was of a her she while she she at all has been been at she - her cari was been at was has her [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(vectors_to_indices(latents_final[0], embedding))\n",
    "# print(tokenizer.decode(vectors_to_indices(latents[0], embedding)))\n",
    "print(tokenizer.decode(sample_text(latents_final, 1)[0]))\n",
    "print(tokenizer.decode(vectors_to_indices(latents_final[0], embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors corresponding to input indices:\n",
      "tensor([[-7.7292e-02,  1.1234e+00, -1.1162e+00,  9.5290e-01,  7.0411e-01,\n",
      "          9.3934e-02,  1.2905e-01, -6.1421e-01, -4.7354e-01,  1.8669e+00,\n",
      "          1.3230e+00,  7.4839e-01,  3.6166e-01, -7.6501e-01, -3.1029e-01,\n",
      "         -1.3262e+00, -1.2330e+00, -2.1209e-01, -1.2452e+00,  6.3154e-01,\n",
      "         -4.2177e-01, -6.7838e-01,  1.8145e-01, -2.4687e-01,  4.7213e-01,\n",
      "          2.9644e-01,  5.5261e-01, -1.4998e+00, -3.2089e-01,  1.9922e+00,\n",
      "         -2.7300e-01, -1.3218e+00, -2.0146e-01,  1.8222e-02, -1.4948e+00,\n",
      "         -5.4760e-01, -3.8630e-01, -6.9837e-01, -1.0270e-01,  8.3724e-01,\n",
      "         -6.1612e-02, -1.1182e+00,  3.0394e+00, -2.8233e-01,  7.6667e-01,\n",
      "         -2.0013e-01,  1.4309e+00, -4.0717e-01, -7.3446e-01,  8.6851e-02],\n",
      "        [-8.8197e-01, -8.1627e-01, -9.9473e-01, -2.0596e-01, -4.3363e-01,\n",
      "         -1.3574e+00,  8.7575e-01,  4.4570e-02,  6.7288e-01, -8.9306e-01,\n",
      "         -5.2451e-01,  6.8276e-02,  9.4779e-01,  8.5183e-01,  1.8238e+00,\n",
      "         -8.1078e-01,  3.2634e-02,  4.0419e-02, -4.5142e-01, -1.1434e+00,\n",
      "          4.7423e-01, -1.3263e+00,  5.2648e-01,  5.5686e-01,  3.4336e-01,\n",
      "         -1.2285e+00, -2.5638e+00, -6.7112e-01, -9.3213e-01,  7.6350e-01,\n",
      "          5.9230e-01,  9.6961e-01,  1.0063e+00, -1.0044e+00,  4.6741e-01,\n",
      "          8.5772e-01, -2.0306e+00,  1.4568e+00,  1.0490e+00, -7.5555e-01,\n",
      "          8.6928e-01,  1.1793e+00, -8.2389e-01,  9.8368e-01, -2.3453e-03,\n",
      "         -3.6195e-01,  6.7647e-02,  9.9563e-01,  4.9906e-01, -6.3485e-01],\n",
      "        [-1.0037e+00,  1.2213e-02,  3.5370e-01,  1.3708e+00,  8.9655e-01,\n",
      "          4.9396e-01,  6.2339e-01, -5.7808e-01,  1.9695e-01, -6.3229e-02,\n",
      "         -4.8823e-01,  1.4844e+00, -7.6281e-01, -1.1291e+00, -1.5135e+00,\n",
      "          7.0173e-01, -5.4904e-01,  6.7051e-01,  7.3987e-01,  8.5169e-01,\n",
      "         -1.5746e+00, -2.1875e-02,  3.7724e-01,  9.0600e-01, -1.3136e+00,\n",
      "         -2.0459e-01,  5.1956e-01, -4.2968e-01,  5.2788e-01,  3.5077e-01,\n",
      "          1.6122e+00,  1.6415e-01, -4.8770e-02, -1.5691e+00, -1.9253e+00,\n",
      "         -2.7641e-01,  2.3915e+00, -5.0840e-01, -5.0205e-01,  3.9166e-01,\n",
      "         -3.8983e+00, -1.4640e-01,  5.3903e-01,  6.1847e-01, -1.7114e-01,\n",
      "          7.0410e-01, -5.9848e-01, -2.4959e+00, -6.8495e-02, -4.1201e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Recovered indices from vectors:\n",
      "tensor([ 3,  7, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the size of vocabulary and embedding dimension\n",
    "vocab_size = 100  # Example vocabulary size\n",
    "embedding_dim = 50  # Example embedding dimension size\n",
    "\n",
    "# Instantiate nn.Embedding module\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Random indices for demonstration purposes\n",
    "indices = torch.tensor([3, 7, 15])  # Example input indices\n",
    "\n",
    "# Convert indices to vectors using the embedding layer\n",
    "vectors = embedding(indices)\n",
    "\n",
    "# Display the vectors corresponding to the input indices\n",
    "print(\"Vectors corresponding to input indices:\")\n",
    "print(vectors)\n",
    "\n",
    "# Function to transform vectors back to indices\n",
    "def vectors_to_indices(vectors, embedding):\n",
    "    # Calculate cosine similarity between vectors and all embedding weights\n",
    "    similarity = torch.matmul(vectors, embedding.weight.T)\n",
    "    \n",
    "    # Get the index of the most similar embedding for each vector\n",
    "    indices = torch.argmax(similarity, dim=1)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Convert vectors back to indices\n",
    "recovered_indices = vectors_to_indices(vectors, embedding)\n",
    "\n",
    "# Display the indices recovered from vectors\n",
    "print(\"\\nRecovered indices from vectors:\")\n",
    "print(recovered_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
