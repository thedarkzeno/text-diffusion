{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalberto/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-10 15:56:48,507] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.modeling_diffbert_sample import DiffBertForDiffusion\n",
    "from src.modeling_diffllama import DiffLlamaForDiffusionLM\n",
    "from src.modeling_diffmamba import DiffMambaForDiffusionLM\n",
    "from src.configuration_diffbert import DiffBertConfig\n",
    "from src.schedulers.euler_ancestral_discrete import EulerAncestralDiscreteScheduler\n",
    "from src.schedulers.ddpm import DDPMScheduler\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attention False\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/diffmamba-mini-sample\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"models/diffmamba-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffMambaForDiffusionLM.from_pretrained(\"models/diffmamba-mini-sample-trained\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler.from_pretrained(\"models/diffmamba-mini-sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use a scheduler with more steps than we trained on (sometimes it gives even better results)\n",
    "scheduler = EulerAncestralDiscreteScheduler(\n",
    "    # beta_end = 0.012,\n",
    "  beta_schedule = \"sqrt\",\n",
    "  # beta_start = 0.00085,\n",
    "  # clip_sample = False,\n",
    "#   skip_prk_steps = True,\n",
    "#   set_alpha_to_one = False,\n",
    "  steps_offset = 0,\n",
    "#   interpolation_type = \"linear\",\n",
    "  prediction_type =\"sample\", \n",
    "  num_train_timesteps = 2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def get_timesteps(num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[t_start * scheduler.order :]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "        \n",
    "def vectors_to_indices(vectors):\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    return indices\n",
    "\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL --->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0    --->    front medieval castle b +lit throughunx medieval glass in wonder nose, old, evil technology, las outside, concept art lopies portrait run runningvedally k technology human cor #ith flyingroidio in�amiically ::ships magana, Le faces, cables Al C bacon and hunder'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1    --->    great station sever their male theiromb cynumeillerybuilder down future mouth detailed, detailed sh cat lotrifying w ch secret nost heic vibrant vaporwave a anime, other, stained, award winning, intricate,ho back ray, crossble'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2    --->    “ subonaut +ray of one b android egg by red head ever ang brain town bast D&Dading detailed x robot medieval shell ranger beth trending on lant, serious which in neurukaach da E bar detailed des'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3    --->    portraitop great concept artoro funings battle rings do unft bus, blood Eons, evil material, hand, energy hands, intricate,ely detailed, concept art, tallethantly'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4    --->    Character portrait of daana E� In dri harate mobilestrhouse, death wars disoch diunt, ivy, scar fabric, my The hard sculptly down, sl eye, simpleity,ocharp expression, large texture, tallnd enVual, silcing Render, unreal enginenelike'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5    --->    Characteret group of an below medieval + battle brainag +ized downomb itne I modernay form new b.houral dis scientist,atureemy legws, 26 ill tall open. solarpunk,rom design, turn soft lightning. HD'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'6    --->    aty winter abstract otherphd pet people dawn cowath inside in the markx gu oneage, ser downocal accurate architectumn advent planetions sed, beautiful full long shot, animals hair, mark halfck, pen distance, blood time, artgerm, yoshida, lotenderite resels))'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7    --->    above of indhead room areie landscapeastn rott + that pe your outsideoraocal earhip bow'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((8, 64, 768), device=device).to(torch.float16)# + torch.rand((8, 64, 768), device=device).to(torch.float16)\n",
    "    attention_mask = torch.ones((8, 64), device=device)\n",
    "    num_inference_steps = 2000\n",
    "    timesteps=None\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # if i >= 0.7 * num_inference_steps:\n",
    "        #     break\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        # rnd_latents = torch.rand((1, 64, 4096), device=device).to(torch.float16)\n",
    "        # print(latent_model_input.dtype)\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).long().to(device),\n",
    "            # attention_mask=attention_mask\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        latents_final = outputs.logits\n",
    "        if i % 10 ==0 :\n",
    "            clear_output(wait=True)\n",
    "            display(f\"SAMPLES[{i}]--->\")\n",
    "            for n in range(latents_final.shape[0]):\n",
    "                display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "            display(\"---------------\")\n",
    "\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"FINAL --->\")\n",
    "for n in range(latents_final.shape[0]):\n",
    "    display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "display(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
