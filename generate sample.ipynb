{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalberto/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-08 10:19:44,354] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.modeling_diffbert_sample import DiffBertForDiffusion\n",
    "from src.modeling_diffllama import DiffLlamaForDiffusionLM\n",
    "from src.modeling_diffmamba import DiffMambaForDiffusionLM\n",
    "from src.configuration_diffbert import DiffBertConfig\n",
    "from src.schedulers.euler_ancestral_discrete import EulerAncestralDiscreteScheduler\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'clip_sample': False, 'interpolation_type': 'linear', 'sample_max_value': 1.0, 'set_alpha_to_one': False, 'sigma_max': None, 'sigma_min': None, 'skip_prk_steps': True, 'timestep_type': 'discrete', 'use_karras_sigmas': False} were passed to EulerAncestralDiscreteScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/diffmamba-mini-sample\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"models/diffmamba-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffMambaForDiffusionLM.from_pretrained(\"models/diffmamba-mini-sample-trained-good\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use a scheduler with more steps than we trained on (sometimes it gives even better results)\n",
    "scheduler = EulerAncestralDiscreteScheduler(\n",
    "    beta_end = 0.012,\n",
    "  beta_schedule = \"scaled_linear\",\n",
    "  beta_start = 0.00085,\n",
    "  # clip_sample = False,\n",
    "#   skip_prk_steps = True,\n",
    "#   set_alpha_to_one = False,\n",
    "  steps_offset = 1,\n",
    "#   interpolation_type = \"linear\",\n",
    "  prediction_type =\"sample\", \n",
    "  num_train_timesteps = 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def get_timesteps(num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[t_start * scheduler.order :]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "        \n",
    "def vectors_to_indices(vectors):\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    return indices\n",
    "\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL --->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0    --->    st girl in portrait of em cat, art by artgerm and greg rutkowski and alphonse mucha'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1    --->    a portrait of a beautiful princess, trending on artstation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2    --->    a portrait of a art by greg rutkowski, concept art, trending on artstation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3    --->    a realistic, concept, trending on artstation,  highly detailed, photorealistic, octane render, 8 k'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4    --->    a cute fine portrait'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5    --->    portrait of a female in a man with tatto by face, holding a giant, hyper detailed, intricate, elegant, art, highly detailed, digital painting, smooth, sharp focus, illustration, cinematic, artstation, concept art'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'6    --->    a portrait of a woman , dramatic lighting, detailed, intricate, elegant, highly detailed, trending on artstation, concept art, sharp focus, beautiful,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7    --->    a ro s digital concept art by studio ghibli'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((8, 64, 768), device=device).to(torch.float16) + torch.rand((8, 64, 768), device=device).to(torch.float16)\n",
    "    attention_mask = torch.ones((8, 64), device=device)\n",
    "    num_inference_steps = 999\n",
    "    timesteps=None\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # if i >= 0.7 * num_inference_steps:\n",
    "        #     break\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        rnd_latents = torch.rand((1, 64, 4096), device=device).to(torch.float16)\n",
    "\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).long().to(device),\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        latents_final = outputs.logits\n",
    "        if i % 10 ==0 :\n",
    "            clear_output(wait=True)\n",
    "            display(f\"SAMPLES[{i}]--->\")\n",
    "            for n in range(latents_final.shape[0]):\n",
    "                display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "            display(\"---------------\")\n",
    "\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"FINAL --->\")\n",
    "for n in range(latents_final.shape[0]):\n",
    "    display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "display(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
