{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDPMScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from modeling_diffbert import DiffBertForDiffusion\n",
    "from configuration_diffbert import DiffBertConfig\n",
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "scheduler = DDPMScheduler()\n",
    "model = DiffBertForDiffusion.from_pretrained(\"diffbert-mini-trained\").to(\"cuda\")\n",
    "device = model.device\n",
    "embedding = BertForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\").to(device)#torch.nn.Embedding(model.config.vocab_size, model.config.hidden_size).to(device)\n",
    "# embedding.load_state_dict(torch.load('diffbert-mini/embedding_weights.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def id_to_one_hot(token_ids, vocab_size=tokenizer.vocab_size):\n",
    "    one_hot_vectors = []\n",
    "    for token_id in token_ids:\n",
    "        # Create a zero-filled array with length equal to vocab_size\n",
    "        one_hot = torch.zeros(vocab_size)\n",
    "        # Set the value at the index of the token ID to 1\n",
    "        one_hot[token_id] = 1\n",
    "        one_hot_vectors.append(one_hot)\n",
    "    return torch.stack(one_hot_vectors, dim=0)\n",
    "\n",
    "def get_max_indices(list_of_tensors):\n",
    "    max_indices = []\n",
    "    for tensor in list_of_tensors:\n",
    "        # Get the index of the maximum value in the tensor\n",
    "        index = torch.argmax(tensor).item()\n",
    "        max_indices.append(index)\n",
    "    return max_indices\n",
    "# Function to transform vectors back to indices\n",
    "def vectors_to_indices(vectors, embedding):\n",
    "    # Calculate cosine similarity between vectors and all embedding weights\n",
    "    # similarity = torch.matmul(vectors, embedding.weight.T)\n",
    "    \n",
    "    # Get the index of the most similar embedding for each vector\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9917, 0.8917, 0.9752,  ..., 0.7484, 0.3251, 0.6549],\n",
      "         [0.6738, 0.7837, 0.0025,  ..., 0.3245, 0.9077, 0.2541],\n",
      "         [0.1522, 0.2947, 0.5039,  ..., 0.4281, 0.8483, 0.2067],\n",
      "         ...,\n",
      "         [0.4833, 0.7358, 0.7627,  ..., 0.1851, 0.8924, 0.6217],\n",
      "         [0.6556, 0.5091, 0.5788,  ..., 0.4632, 0.3302, 0.4513],\n",
      "         [0.9822, 0.6312, 0.8541,  ..., 0.2629, 0.1083, 0.2390]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba2655e16e4abdb9a9d9b4c16c2b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "latents = torch.rand((1, 64, 768), device=device)\n",
    "print(latents)\n",
    "num_inference_steps = 1000\n",
    "timesteps=None#[999, 500, 1]\n",
    "timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "# print(timesteps)\n",
    "for i, t in tqdm(enumerate(timesteps)):\n",
    "    # expand the latents if we are doing classifier free guidance\n",
    "    latent_model_input =  latents\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "    # predict the noise residual\n",
    "    noise_pred = model(\n",
    "        inputs_embeds=latent_model_input,\n",
    "        timesteps=t.reshape(1,).to(device),\n",
    "        # encoder_hidden_states=prompt_embeds,\n",
    "        # timestep_cond=timestep_cond,\n",
    "        # cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "        # added_cond_kwargs=added_cond_kwargs,\n",
    "        # return_dict=False,\n",
    "    ).logits\n",
    "\n",
    "\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_final = embedding.cls(latents)\n",
    "# embedding.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4463, 13926, 12066,  2448, 17862, 17862, 20940, 16753, 16431, 15087,\n",
      "        17862, 16691, 20695,   446,  8735, 18492, 17292,  7061, 15501, 20940,\n",
      "          625,   117, 21471, 18012,   117,  6855, 12660,  2820,  6190,  1548,\n",
      "        14069, 17862, 14069,  1003, 18280,  1587,  5311, 17862,  6855, 16753,\n",
      "         1291, 17862,   117, 20314, 15558, 20912, 17862, 18011,  1229,  6855,\n",
      "        18420,  2438,  6855,  6855,  6855, 16306,  2513, 11989, 14285,  6855,\n",
      "         3483,  6855,  2933, 19692], device='cuda:0')\n",
      "##âmbmund model son reality reality surfcript Ever from reality cláusborgingpot Pokémon Premier WW gé surf quando, tib fara, frequent vamp design excel class rup reality rup vis Kiss baswer reality frequentcriptional reality,úngrive visco realityeight super frequent abus id frequent frequent frequentngerater rin Tara frequent guitar frequent minurough\n"
     ]
    }
   ],
   "source": [
    "print(vectors_to_indices(latents_final[0], embedding))\n",
    "# print(tokenizer.decode(vectors_to_indices(latents[0], embedding)))\n",
    "print(tokenizer.decode(vectors_to_indices(latents_final[0], embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors corresponding to input indices:\n",
      "tensor([[-7.7292e-02,  1.1234e+00, -1.1162e+00,  9.5290e-01,  7.0411e-01,\n",
      "          9.3934e-02,  1.2905e-01, -6.1421e-01, -4.7354e-01,  1.8669e+00,\n",
      "          1.3230e+00,  7.4839e-01,  3.6166e-01, -7.6501e-01, -3.1029e-01,\n",
      "         -1.3262e+00, -1.2330e+00, -2.1209e-01, -1.2452e+00,  6.3154e-01,\n",
      "         -4.2177e-01, -6.7838e-01,  1.8145e-01, -2.4687e-01,  4.7213e-01,\n",
      "          2.9644e-01,  5.5261e-01, -1.4998e+00, -3.2089e-01,  1.9922e+00,\n",
      "         -2.7300e-01, -1.3218e+00, -2.0146e-01,  1.8222e-02, -1.4948e+00,\n",
      "         -5.4760e-01, -3.8630e-01, -6.9837e-01, -1.0270e-01,  8.3724e-01,\n",
      "         -6.1612e-02, -1.1182e+00,  3.0394e+00, -2.8233e-01,  7.6667e-01,\n",
      "         -2.0013e-01,  1.4309e+00, -4.0717e-01, -7.3446e-01,  8.6851e-02],\n",
      "        [-8.8197e-01, -8.1627e-01, -9.9473e-01, -2.0596e-01, -4.3363e-01,\n",
      "         -1.3574e+00,  8.7575e-01,  4.4570e-02,  6.7288e-01, -8.9306e-01,\n",
      "         -5.2451e-01,  6.8276e-02,  9.4779e-01,  8.5183e-01,  1.8238e+00,\n",
      "         -8.1078e-01,  3.2634e-02,  4.0419e-02, -4.5142e-01, -1.1434e+00,\n",
      "          4.7423e-01, -1.3263e+00,  5.2648e-01,  5.5686e-01,  3.4336e-01,\n",
      "         -1.2285e+00, -2.5638e+00, -6.7112e-01, -9.3213e-01,  7.6350e-01,\n",
      "          5.9230e-01,  9.6961e-01,  1.0063e+00, -1.0044e+00,  4.6741e-01,\n",
      "          8.5772e-01, -2.0306e+00,  1.4568e+00,  1.0490e+00, -7.5555e-01,\n",
      "          8.6928e-01,  1.1793e+00, -8.2389e-01,  9.8368e-01, -2.3453e-03,\n",
      "         -3.6195e-01,  6.7647e-02,  9.9563e-01,  4.9906e-01, -6.3485e-01],\n",
      "        [-1.0037e+00,  1.2213e-02,  3.5370e-01,  1.3708e+00,  8.9655e-01,\n",
      "          4.9396e-01,  6.2339e-01, -5.7808e-01,  1.9695e-01, -6.3229e-02,\n",
      "         -4.8823e-01,  1.4844e+00, -7.6281e-01, -1.1291e+00, -1.5135e+00,\n",
      "          7.0173e-01, -5.4904e-01,  6.7051e-01,  7.3987e-01,  8.5169e-01,\n",
      "         -1.5746e+00, -2.1875e-02,  3.7724e-01,  9.0600e-01, -1.3136e+00,\n",
      "         -2.0459e-01,  5.1956e-01, -4.2968e-01,  5.2788e-01,  3.5077e-01,\n",
      "          1.6122e+00,  1.6415e-01, -4.8770e-02, -1.5691e+00, -1.9253e+00,\n",
      "         -2.7641e-01,  2.3915e+00, -5.0840e-01, -5.0205e-01,  3.9166e-01,\n",
      "         -3.8983e+00, -1.4640e-01,  5.3903e-01,  6.1847e-01, -1.7114e-01,\n",
      "          7.0410e-01, -5.9848e-01, -2.4959e+00, -6.8495e-02, -4.1201e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Recovered indices from vectors:\n",
      "tensor([ 3,  7, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the size of vocabulary and embedding dimension\n",
    "vocab_size = 100  # Example vocabulary size\n",
    "embedding_dim = 50  # Example embedding dimension size\n",
    "\n",
    "# Instantiate nn.Embedding module\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Random indices for demonstration purposes\n",
    "indices = torch.tensor([3, 7, 15])  # Example input indices\n",
    "\n",
    "# Convert indices to vectors using the embedding layer\n",
    "vectors = embedding(indices)\n",
    "\n",
    "# Display the vectors corresponding to the input indices\n",
    "print(\"Vectors corresponding to input indices:\")\n",
    "print(vectors)\n",
    "\n",
    "# Function to transform vectors back to indices\n",
    "def vectors_to_indices(vectors, embedding):\n",
    "    # Calculate cosine similarity between vectors and all embedding weights\n",
    "    similarity = torch.matmul(vectors, embedding.weight.T)\n",
    "    \n",
    "    # Get the index of the most similar embedding for each vector\n",
    "    indices = torch.argmax(similarity, dim=1)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Convert vectors back to indices\n",
    "recovered_indices = vectors_to_indices(vectors, embedding)\n",
    "\n",
    "# Display the indices recovered from vectors\n",
    "print(\"\\nRecovered indices from vectors:\")\n",
    "print(recovered_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
