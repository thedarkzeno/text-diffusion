{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalberto/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-11 01:06:13,065] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.denoisers.modeling_diffmamba import DiffMambaForDiffusionLM\n",
    "from src.decoders.bert_decoder import BertLMHeadModel\n",
    "from src.schedulers.euler_ancestral_discrete import EulerAncestralDiscreteScheduler\n",
    "from src.schedulers.ddpm import DDPMScheduler\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attention False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"models/diffmamba-mini-sample-trained\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, subfolder=\"tokenizer\")\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(path, subfolder=\"scheduler\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffMambaForDiffusionLM.from_pretrained(path, torch_dtype=torch.float16, subfolder=\"denoiser\").to(\"cuda\")\n",
    "decoder = BertLMHeadModel.from_pretrained(path, torch_dtype=torch.float16, subfolder=\"decoder\").to(\"cuda\")\n",
    "\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalberto/.local/lib/python3.8/site-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'EulerAncestralDiscreteScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'EulerAncestralDiscreteScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.num_train_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def get_timesteps(num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[t_start * scheduler.order :]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "        \n",
    "def vectors_to_indices(vectors):\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    return indices\n",
    "\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL --->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0    --->    germut of on body concept, greg r and and b, hyper realistic, asantant e concepty, h by j her byineer and by digital, intr on on artstation, h,, Art, engine, style'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1    --->    a of- hairistic, highly dal hair, 3 fantasy, intricate, elegant loeteicate intricate detailed landscape l a wicateu, h details and hingane by by female byseonaing vsephonin, h,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((2, 64, 768), device=device).to(torch.float16)# + torch.rand((8, 128, 768), device=device).to(torch.float16)\n",
    "    attention_mask = torch.ones((2, 64), device=device)\n",
    "    num_inference_steps = scheduler.num_train_timesteps\n",
    "    timesteps=None\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # if i >= 0.7 * num_inference_steps:\n",
    "        #     break\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        # rnd_latents = torch.rand((1, 64, 4096), device=device).to(torch.float16)\n",
    "        # print(latent_model_input.dtype)\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).long().to(device),\n",
    "            # attention_mask=attention_mask\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        latents_final = outputs.logits\n",
    "        if i % 10 ==0 :\n",
    "            clear_output(wait=True)\n",
    "            display(f\"SAMPLES[{i}]--->\")\n",
    "            for n in range(latents_final.shape[0]):\n",
    "                display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "            display(\"---------------\")\n",
    "\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"FINAL --->\")\n",
    "for n in range(latents_final.shape[0]):\n",
    "    display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "display(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 32001])\n",
      "1395\n",
      "gre\n",
      "torch.Size([2, 2, 32001])\n",
      "29887\n",
      "g\n",
      "torch.Size([2, 3, 32001])\n",
      "364\n",
      "r\n",
      "torch.Size([2, 4, 32001])\n",
      "329\n",
      "ut\n",
      "torch.Size([2, 5, 32001])\n",
      "7000\n",
      "kow\n",
      "torch.Size([2, 6, 32001])\n",
      "2574\n",
      "ski\n",
      "torch.Size([2, 7, 32001])\n",
      "322\n",
      "and\n",
      "torch.Size([2, 8, 32001])\n",
      "394\n",
      "al\n",
      "torch.Size([2, 9, 32001])\n",
      "17607\n",
      "phon\n",
      "torch.Size([2, 10, 32001])\n",
      "344\n",
      "se\n",
      "torch.Size([2, 11, 32001])\n",
      "1568\n",
      "much\n",
      "torch.Size([2, 12, 32001])\n",
      "29874\n",
      "a\n",
      "torch.Size([2, 13, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 14, 32001])\n",
      "330\n",
      "g\n",
      "torch.Size([2, 15, 32001])\n",
      "677\n",
      "low\n",
      "torch.Size([2, 16, 32001])\n",
      "292\n",
      "ing\n",
      "torch.Size([2, 17, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 18, 32001])\n",
      "330\n",
      "g\n",
      "torch.Size([2, 19, 32001])\n",
      "677\n",
      "low\n",
      "torch.Size([2, 20, 32001])\n",
      "292\n",
      "ing\n",
      "torch.Size([2, 21, 32001])\n",
      "26068\n",
      "lights\n",
      "torch.Size([2, 22, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 23, 32001])\n",
      "534\n",
      "tr\n",
      "torch.Size([2, 24, 32001])\n",
      "2548\n",
      "ending\n",
      "torch.Size([2, 25, 32001])\n",
      "373\n",
      "on\n",
      "torch.Size([2, 26, 32001])\n",
      "1616\n",
      "art\n",
      "torch.Size([2, 27, 32001])\n",
      "19569\n",
      "station\n",
      "torch.Size([2, 28, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 29, 32001])\n",
      "491\n",
      "by\n",
      "torch.Size([2, 30, 32001])\n",
      "1395\n",
      "gre\n",
      "torch.Size([2, 31, 32001])\n",
      "29887\n",
      "g\n",
      "torch.Size([2, 32, 32001])\n",
      "364\n",
      "r\n",
      "torch.Size([2, 33, 32001])\n",
      "329\n",
      "ut\n",
      "torch.Size([2, 34, 32001])\n",
      "7000\n",
      "kow\n",
      "torch.Size([2, 35, 32001])\n",
      "2574\n",
      "ski\n",
      "torch.Size([2, 36, 32001])\n",
      "322\n",
      "and\n",
      "torch.Size([2, 37, 32001])\n",
      "394\n",
      "al\n",
      "torch.Size([2, 38, 32001])\n",
      "17607\n",
      "phon\n",
      "torch.Size([2, 39, 32001])\n",
      "344\n",
      "se\n",
      "torch.Size([2, 40, 32001])\n",
      "1568\n",
      "much\n",
      "torch.Size([2, 41, 32001])\n",
      "29874\n",
      "a\n",
      "torch.Size([2, 42, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 43, 32001])\n",
      "394\n",
      "al\n",
      "torch.Size([2, 44, 32001])\n",
      "17607\n",
      "phon\n",
      "torch.Size([2, 45, 32001])\n",
      "344\n",
      "se\n",
      "torch.Size([2, 46, 32001])\n",
      "1568\n",
      "much\n",
      "torch.Size([2, 47, 32001])\n",
      "29874\n",
      "a\n",
      "torch.Size([2, 48, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 49, 32001])\n",
      "696\n",
      "ro\n",
      "torch.Size([2, 50, 32001])\n",
      "893\n",
      "ss\n",
      "torch.Size([2, 51, 32001])\n",
      "4012\n",
      "draw\n",
      "torch.Size([2, 52, 32001])\n",
      "29879\n",
      "s\n",
      "torch.Size([2, 53, 32001])\n",
      "29892\n",
      ",\n",
      "torch.Size([2, 54, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 55, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 56, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 57, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 58, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 59, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 60, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 61, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 62, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 63, 32001])\n",
      "32000\n",
      "<pad>\n",
      "torch.Size([2, 64, 32001])\n",
      "32000\n",
      "<pad>\n",
      "greg rutkowski and alphonse mucha, glowing, glowing lights, trending on artstation, by greg rutkowski and alphonse mucha, alphonse mucha, rossdraws,<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "# # print(inputs.input_ids)\n",
    "# bsz, seq_ln = inputs.input_ids.shape\n",
    "\n",
    "# latents = torch.rand((1, 64, 768), device=device).to(torch.float16)\n",
    "encoder_hidden_states = latents\n",
    "decoder_input_ids = [0]\n",
    "predicted_ids = []\n",
    "for i in range(64): \n",
    "    outputs = decoder(input_ids=torch.tensor(([decoder_input_ids])).to(model.device), encoder_hidden_states=encoder_hidden_states)\n",
    "    print(outputs.logits.shape)\n",
    "    logits = outputs.logits[:,i,:]\n",
    "    # perform argmax on the last dimension (i.e. greedy decoding)\n",
    "    predicted_id = logits.argmax(-1)\n",
    "    print(predicted_id[0].item())\n",
    "    predicted_ids.append(predicted_id[0].item())\n",
    "    print(tokenizer.decode([predicted_id[0].squeeze()]))\n",
    "    # add predicted id to decoder_input_ids\n",
    "    decoder_input_ids = decoder_input_ids + [predicted_id[0].item()]\n",
    "print(tokenizer.decode(predicted_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32001])\n",
      "385 tensor([[385, 319]], device='cuda:0')\n",
      "385\n",
      "an\n",
      "torch.Size([1, 2, 32001])\n",
      "603 tensor([[ 603, 8678]], device='cuda:0')\n",
      "603\n",
      "ime\n",
      "torch.Size([1, 3, 32001])\n",
      "21760 tensor([[21760,  1820]], device='cuda:0')\n",
      "21760\n",
      "portrait\n",
      "torch.Size([1, 4, 32001])\n",
      "29892 tensor([[29892,   310]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 5, 32001])\n",
      "5094 tensor([[5094, 1472]], device='cuda:0')\n",
      "5094\n",
      "cy\n",
      "torch.Size([1, 6, 32001])\n",
      "495 tensor([[  495, 14203]], device='cuda:0')\n",
      "495\n",
      "ber\n",
      "torch.Size([1, 7, 32001])\n",
      "29886 tensor([[29886,  1212]], device='cuda:0')\n",
      "29886\n",
      "p\n",
      "torch.Size([1, 8, 32001])\n",
      "2960 tensor([[ 2960, 29892]], device='cuda:0')\n",
      "2960\n",
      "unk\n",
      "torch.Size([1, 9, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 10, 32001])\n",
      "5094 tensor([[5094, 1472]], device='cuda:0')\n",
      "5094\n",
      "cy\n",
      "torch.Size([1, 11, 32001])\n",
      "495 tensor([[  495, 14203]], device='cuda:0')\n",
      "495\n",
      "ber\n",
      "torch.Size([1, 12, 32001])\n",
      "29886 tensor([[29886,  1212]], device='cuda:0')\n",
      "29886\n",
      "p\n",
      "torch.Size([1, 13, 32001])\n",
      "2960 tensor([[ 2960, 29892]], device='cuda:0')\n",
      "2960\n",
      "unk\n",
      "torch.Size([1, 14, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 15, 32001])\n",
      "11158 tensor([[11158,   330]], device='cuda:0')\n",
      "11158\n",
      "intr\n",
      "torch.Size([1, 16, 32001])\n",
      "9593 tensor([[9593,  293]], device='cuda:0')\n",
      "9593\n",
      "icate\n",
      "torch.Size([1, 17, 32001])\n",
      "29892 tensor([[29892,  4902]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 18, 32001])\n",
      "19232 tensor([[19232, 10712]], device='cuda:0')\n",
      "19232\n",
      "elegant\n",
      "torch.Size([1, 19, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 20, 32001])\n",
      "10712 tensor([[10712, 13436]], device='cuda:0')\n",
      "10712\n",
      "highly\n",
      "torch.Size([1, 21, 32001])\n",
      "13173 tensor([[13173, 29892]], device='cuda:0')\n",
      "13173\n",
      "detailed\n",
      "torch.Size([1, 22, 32001])\n",
      "29892 tensor([[29892,  3700]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 23, 32001])\n",
      "13436 tensor([[13436, 24764]], device='cuda:0')\n",
      "13436\n",
      "digital\n",
      "torch.Size([1, 24, 32001])\n",
      "20413 tensor([[20413, 21760]], device='cuda:0')\n",
      "20413\n",
      "painting\n",
      "torch.Size([1, 25, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 26, 32001])\n",
      "1616 tensor([[1616,  534]], device='cuda:0')\n",
      "1616\n",
      "art\n",
      "torch.Size([1, 27, 32001])\n",
      "19569 tensor([[19569, 29892]], device='cuda:0')\n",
      "19569\n",
      "station\n",
      "torch.Size([1, 28, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 29, 32001])\n",
      "6964 tensor([[ 6964, 10597]], device='cuda:0')\n",
      "6964\n",
      "concept\n",
      "torch.Size([1, 30, 32001])\n",
      "1616 tensor([[ 1616, 29892]], device='cuda:0')\n",
      "1616\n",
      "art\n",
      "torch.Size([1, 31, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 32, 32001])\n",
      "10597 tensor([[10597,  1775]], device='cuda:0')\n",
      "10597\n",
      "smooth\n",
      "torch.Size([1, 33, 32001])\n",
      "29892 tensor([[29892,  4050]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 34, 32001])\n",
      "15301 tensor([[15301, 10597]], device='cuda:0')\n",
      "15301\n",
      "sharp\n",
      "torch.Size([1, 35, 32001])\n",
      "8569 tensor([[ 8569, 29892]], device='cuda:0')\n",
      "8569\n",
      "focus\n",
      "torch.Size([1, 36, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 37, 32001])\n",
      "8632 tensor([[8632, 4670]], device='cuda:0')\n",
      "8632\n",
      "illustr\n",
      "torch.Size([1, 38, 32001])\n",
      "362 tensor([[  362, 29890]], device='cuda:0')\n",
      "362\n",
      "ation\n",
      "torch.Size([1, 39, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 40, 32001])\n",
      "1395 tensor([[1395, 6454]], device='cuda:0')\n",
      "1395\n",
      "gre\n",
      "torch.Size([1, 41, 32001])\n",
      "29887 tensor([[29887, 29892]], device='cuda:0')\n",
      "29887\n",
      "g\n",
      "torch.Size([1, 42, 32001])\n",
      "364 tensor([[ 364, 2839]], device='cuda:0')\n",
      "364\n",
      "r\n",
      "torch.Size([1, 43, 32001])\n",
      "329 tensor([[ 329, 4061]], device='cuda:0')\n",
      "329\n",
      "ut\n",
      "torch.Size([1, 44, 32001])\n",
      "7000 tensor([[7000,  332]], device='cuda:0')\n",
      "7000\n",
      "kow\n",
      "torch.Size([1, 45, 32001])\n",
      "2574 tensor([[ 2574, 29892]], device='cuda:0')\n",
      "2574\n",
      "ski\n",
      "torch.Size([1, 46, 32001])\n",
      "29892 tensor([[29892, 29889]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 47, 32001])\n",
      "6454 tensor([[6454,  282]], device='cuda:0')\n",
      "6454\n",
      "tom\n",
      "torch.Size([1, 48, 32001])\n",
      "12617 tensor([[12617,   289]], device='cuda:0')\n",
      "12617\n",
      "asz\n",
      "torch.Size([1, 49, 32001])\n",
      "394 tensor([[394, 838]], device='cuda:0')\n",
      "394\n",
      "al\n",
      "torch.Size([1, 50, 32001])\n",
      "17607 tensor([[17607,   264]], device='cuda:0')\n",
      "17607\n",
      "phon\n",
      "torch.Size([1, 51, 32001])\n",
      "344 tensor([[  344, 29892]], device='cuda:0')\n",
      "344\n",
      "se\n",
      "torch.Size([1, 52, 32001])\n",
      "1568 tensor([[ 1568, 18927]], device='cuda:0')\n",
      "1568\n",
      "much\n",
      "torch.Size([1, 53, 32001])\n",
      "29874 tensor([[29874, 29892]], device='cuda:0')\n",
      "29874\n",
      "a\n",
      "torch.Size([1, 54, 32001])\n",
      "32000 tensor([[32000, 29892]], device='cuda:0')\n",
      "29892\n",
      ",\n",
      "torch.Size([1, 55, 32001])\n",
      "282 tensor([[  282, 32000]], device='cuda:0')\n",
      "282\n",
      "p\n",
      "torch.Size([1, 56, 32001])\n",
      "1308 tensor([[1308, 1639]], device='cuda:0')\n",
      "1308\n",
      "eter\n",
      "torch.Size([1, 57, 32001])\n",
      "2730 tensor([[ 2730, 32000]], device='cuda:0')\n",
      "2730\n",
      "mo\n",
      "torch.Size([1, 58, 32001])\n",
      "1092 tensor([[1092,  774]], device='cuda:0')\n",
      "1092\n",
      "hr\n",
      "torch.Size([1, 59, 32001])\n",
      "6740 tensor([[ 6740, 32000]], device='cuda:0')\n",
      "6740\n",
      "bach\n",
      "torch.Size([1, 60, 32001])\n",
      "261 tensor([[  261, 32000]], device='cuda:0')\n",
      "261\n",
      "er\n",
      "torch.Size([1, 61, 32001])\n",
      "32000 tensor([[32000,   296]], device='cuda:0')\n",
      "296\n",
      "ent\n",
      "torch.Size([1, 62, 32001])\n",
      "32000 tensor([[32000, 29889]], device='cuda:0')\n",
      "29889\n",
      ".\n",
      "torch.Size([1, 63, 32001])\n",
      "32000 tensor([[32000,   298]], device='cuda:0')\n",
      "298\n",
      "h\n",
      "torch.Size([1, 64, 32001])\n",
      "29881 tensor([[29881,  7154]], device='cuda:0')\n",
      "29881\n",
      "d\n",
      "anime portrait, cyberpunk, cyberpunk, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, greg rutkowski, tomasz alphonse mucha, peter mohrbacherent. hd\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "# # print(inputs.input_ids)\n",
    "encoder_hidden_states = latents\n",
    "\n",
    "decoder_input_ids = [0]\n",
    "predicted_ids = []\n",
    "for i in range(64): \n",
    "    outputs = decoder(input_ids=torch.tensor([decoder_input_ids]).to(model.device), encoder_hidden_states=encoder_hidden_states)\n",
    "    print(outputs.logits.shape)\n",
    "    logits = outputs.logits[:, i, :]\n",
    "    # Handling 32000 token\n",
    "    argmax_value = logits.argmax(-1)\n",
    "    top_logits, top_indices = logits.topk(2, dim=-1)\n",
    "    \n",
    "    print(argmax_value.item(), top_indices)\n",
    "    predicted_id = argmax_value.item() if argmax_value.item() != 32000 else top_indices[0][1].item()\n",
    "    print(predicted_id)\n",
    "    predicted_ids.append(predicted_id)\n",
    "    print(tokenizer.decode([predicted_id]))\n",
    "    # add predicted id to decoder_input_ids\n",
    "    decoder_input_ids = decoder_input_ids + [predicted_id]\n",
    "print(tokenizer.decode(predicted_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
