{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalberto/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-10 10:52:05,202] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.modeling_diffbert_sample import DiffBertForDiffusion\n",
    "from src.modeling_diffllama import DiffLlamaForDiffusionLM\n",
    "from src.modeling_diffmamba import DiffMambaForDiffusionLM\n",
    "from src.configuration_diffbert import DiffBertConfig\n",
    "from src.schedulers.euler_ancestral_discrete import EulerAncestralDiscreteScheduler\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attention True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/diffmamba-mini-sample-trained-good\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"models/diffmamba-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffMambaForDiffusionLM.from_pretrained(\"models/diffmamba-mini-sample-trained\", add_cross_attention=True, torch_dtype=torch.float16).to(\"cuda\")\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('num_train_timesteps', 1000),\n",
       "            ('beta_start', 0.0001),\n",
       "            ('beta_end', 0.02),\n",
       "            ('beta_schedule', 'linear'),\n",
       "            ('trained_betas', None),\n",
       "            ('prediction_type', 'sample'),\n",
       "            ('timestep_spacing', 'leading'),\n",
       "            ('steps_offset', 0),\n",
       "            ('_class_name', 'DDIMScheduler'),\n",
       "            ('_diffusers_version', '0.23.1'),\n",
       "            ('clip_sample', True),\n",
       "            ('clip_sample_range', 1.0),\n",
       "            ('dynamic_thresholding_ratio', 0.995),\n",
       "            ('rescale_betas_zero_snr', False),\n",
       "            ('sample_max_value', 1.0),\n",
       "            ('set_alpha_to_one', True),\n",
       "            ('thresholding', False)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scheduler = DDIMScheduler.from_pretrained(\"models/diffmamba-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "scheduler.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = EulerAncestralDiscreteScheduler(\n",
    "    beta_end = 0.012,\n",
    "  beta_schedule = \"scaled_linear\",\n",
    "  beta_start = 0.00085,\n",
    "  # clip_sample = False,\n",
    "#   skip_prk_steps = True,\n",
    "#   set_alpha_to_one = False,\n",
    "  # steps_offset = 1,\n",
    "#   interpolation_type = \"linear\",\n",
    "  prediction_type =\"sample\", \n",
    "  num_train_timesteps = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def get_timesteps(num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[t_start * scheduler.order :]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "        \n",
    "def vectors_to_indices(vectors):\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    return indices\n",
    "\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL --->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0    --->    ça que faz aemar pres foi no Estpar eito deccto e nos maioroso de nosve n S mundo e, estarER ou valor, as há grande entre doençavar pedag da autor paravo especial em que e milhaico - tinhaumfero de de tal por'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1    --->    ídosdaados com agências comumasMas de nem emiraado, deências tanto e érlada e podelesção de autor e téc aqulantes- proteçãoviiva mar, dar daem Lade:elem as pessoas doant quanto no deáudo f'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2    --->    é em eucom antesantesamento.em não poder. prevuto porne fção mar, a visua aferriem delicas que de toda a público que e algum com oul dar a l necess está e aquínm mundo deanedadeeu ase do nossoo. '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3    --->    seu e que aqu eiões salvaest garantia- antes antesção com aosl do mesmo, sobre se enxneadora parapr apenas o e que de era são ela a ir que foi aíima de que dia negas aqui, contido a grande com deen aqu e de'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4    --->    heza de faria antes tambémadoabilata de um gistal, Pata ou aqu ambra a bz são falente que forma nuzo a filiic de oviço da imp do com a) e quando de de todo que era social Sra ele e sobre da Eduira'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5    --->    em choca ou motvel entre depois ou olção, nuaperores do público e davelup aivos do emcer e eu conoem ào fadas e a esteo comências, e por estavaender e 3 a de nem de foca palear e de com a'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'6    --->    ú tanto ou emocção de nas menfene o aquelesito de transenaneina para a aqupua ou dosogunE em produção do jocinas de Lvies um jienteras e uma assimes poderetane sem bíza da eriaendo, de man'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7    --->    daval necessant masiententam... aces me estar no ambhoso e ou, de pontosido primeirojá de formarasodado ao quandoânica deixências de paraícios, e até eamentoidas e sim, os um filh é nas mar de poderos'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "batch_size = 8\n",
    "cfg=1\n",
    "prompt = [\"Os biólogos do Zoológico estão realizando um trabalho de enriquecimento ambiental, que é a melhoria do ambiente ou recinto em que vivem os animais, para que fique o mais próximo possível do habitat natural A Secretaria Municipal\"] * batch_size\n",
    "neg_prompt = [\"\"] * batch_size\n",
    "\n",
    "input_ids = tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").to(\"cuda\")\n",
    "neg_input_ids = tokenizer(neg_prompt, padding=\"max_length\", max_length=input_ids.input_ids.shape[1], return_tensors=\"pt\").to(\"cuda\")\n",
    "encoder_hidden_states = model.apply_embeddings(input_ids.input_ids).to(model.dtype)\n",
    "neg_encoder_hidden_states = model.apply_embeddings(neg_input_ids.input_ids).to(model.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((batch_size, input_ids.input_ids.shape[1], 768), device=device).to(model.dtype) + torch.rand((8, input_ids.input_ids.shape[1], 768), device=device).to(torch.float16)\n",
    "    attention_mask = torch.ones((batch_size, input_ids.input_ids.shape[1]), device=device)\n",
    "    num_inference_steps = 1000\n",
    "    timesteps=None\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    \n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # if i >= 0.7 * num_inference_steps:\n",
    "        #     break\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        latent_model_input = torch.cat([latents] * 2) if cfg > 1 else latents\n",
    "        prompt_embeds = torch.cat([encoder_hidden_states, neg_encoder_hidden_states]) if cfg > 1 else encoder_hidden_states\n",
    "\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).long().to(device),\n",
    "            encoder_hidden_states=prompt_embeds\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        if cfg > 1:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + cfg * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        \n",
    "        latents_final = outputs.logits\n",
    "        if i % 10 ==0 :\n",
    "            clear_output(wait=True)\n",
    "            display(f\"SAMPLES[{i}]--->\")\n",
    "            for n in range(latents_final.shape[0]):\n",
    "                display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "            display(\"---------------\")\n",
    "\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"FINAL --->\")\n",
    "for n in range(latents_final.shape[0]):\n",
    "    display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "display(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"isso é um teste\", return_tensors=\"pt\").to(\"cuda\")\n",
    "input_ids.input_ids.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
