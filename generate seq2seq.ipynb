{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalberto/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-09 00:03:40,075] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.modeling_diffbert_sample import DiffBertForDiffusion\n",
    "from src.modeling_diffllama import DiffLlamaForDiffusionLM\n",
    "from src.modeling_diffmamba import DiffMambaForDiffusionLM\n",
    "from src.configuration_diffbert import DiffBertConfig\n",
    "from src.schedulers.euler_ancestral_discrete import EulerAncestralDiscreteScheduler\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# model(inputs_embeds=inputs_embeds, timesteps=timesteps).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attention True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/diffmamba-mini-sample-trained-good\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"models/diffmamba-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "model = DiffMambaForDiffusionLM.from_pretrained(\"models/diffmamba-mini-sample-trained\", add_cross_attention=True, torch_dtype=torch.float16).to(\"cuda\")\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('num_train_timesteps', 1000),\n",
       "            ('beta_start', 0.0001),\n",
       "            ('beta_end', 0.02),\n",
       "            ('beta_schedule', 'linear'),\n",
       "            ('trained_betas', None),\n",
       "            ('clip_sample', True),\n",
       "            ('set_alpha_to_one', True),\n",
       "            ('steps_offset', 0),\n",
       "            ('prediction_type', 'sample'),\n",
       "            ('thresholding', False),\n",
       "            ('dynamic_thresholding_ratio', 0.995),\n",
       "            ('clip_sample_range', 1.0),\n",
       "            ('sample_max_value', 1.0),\n",
       "            ('timestep_spacing', 'leading'),\n",
       "            ('rescale_betas_zero_snr', False),\n",
       "            ('_class_name', 'DDIMScheduler'),\n",
       "            ('_diffusers_version', '0.23.1')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = DDIMScheduler.from_pretrained(\"models/diffmamba-mini-sample\")#DDIMScheduler(prediction_type=\"sample\", num_train_timesteps=2000)\n",
    "scheduler.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDIMScheduler(\n",
    "  #   beta_end = 0.012,\n",
    "  # beta_schedule = \"scaled_linear\",\n",
    "  # beta_start = 0.00085,\n",
    "  # clip_sample = False,\n",
    "#   skip_prk_steps = True,\n",
    "#   set_alpha_to_one = False,\n",
    "  # steps_offset = 1,\n",
    "#   interpolation_type = \"linear\",\n",
    "  prediction_type =\"sample\", \n",
    "  num_train_timesteps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used,\n",
    "            `timesteps` must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default\n",
    "                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`\n",
    "                must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "def get_timesteps(num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = scheduler.timesteps[t_start * scheduler.order :]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "        \n",
    "def vectors_to_indices(vectors):\n",
    "    indices = torch.argmax(vectors, dim=-1)\n",
    "    return indices\n",
    "\n",
    "def sample_text(probabilities, temperature=1.0):\n",
    "    batch_size, seq_len, vocab_size = probabilities.size()\n",
    "    flattened_probs = probabilities.view(batch_size * seq_len, -1)\n",
    "    \n",
    "    scaled_logits = flattened_probs / temperature\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    sampled_indices = torch.multinomial(scaled_probs, 1)\n",
    "    sampled_token_ids = sampled_indices.view(batch_size, seq_len)\n",
    "    \n",
    "    return sampled_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINAL --->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0    --->    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1    --->    ,,,,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2    --->    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3    --->    ,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4    --->    ,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5    --->    ,,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'6    --->    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'7    --->    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "batch_size = 8\n",
    "cfg=1\n",
    "prompt = [\"A melancholic depiction of war with a surrealistic touch, featuring a staircase and weeping statues, illuminated by a sunset and surrounded by ice and intricate details.\"] * batch_size\n",
    "neg_prompt = [\"\"] * batch_size\n",
    "\n",
    "input_ids = tokenizer(prompt, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(\"cuda\")\n",
    "neg_input_ids = tokenizer(neg_prompt, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(\"cuda\")\n",
    "encoder_hidden_states = model.apply_embeddings(input_ids.input_ids).to(model.dtype)\n",
    "neg_encoder_hidden_states = model.apply_embeddings(neg_input_ids.input_ids).to(model.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = torch.rand((batch_size, 64, 768), device=device).to(model.dtype) + torch.rand((8, 64, 768), device=device).to(torch.float16)\n",
    "    attention_mask = torch.ones((batch_size, 64), device=device)\n",
    "    num_inference_steps = 1000\n",
    "    timesteps=None\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    \n",
    "    for i, t in tqdm(enumerate(timesteps)):\n",
    "        # if i >= 0.7 * num_inference_steps:\n",
    "        #     break\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input =  latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "        latent_model_input = torch.cat([latents] * 2) if cfg > 1 else latents\n",
    "        prompt_embeds = torch.cat([encoder_hidden_states, neg_encoder_hidden_states]) if cfg > 1 else encoder_hidden_states\n",
    "\n",
    "        outputs = model(\n",
    "            input_embeds=latent_model_input,\n",
    "            timesteps=t.reshape(1,).long().to(device),\n",
    "            encoder_hidden_states=prompt_embeds\n",
    "        )\n",
    "        noise_pred = outputs.last_hidden_state\n",
    "        if cfg > 1:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + cfg * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        \n",
    "        latents_final = outputs.logits\n",
    "        if i % 10 ==0 :\n",
    "            clear_output(wait=True)\n",
    "            display(f\"SAMPLES[{i}]--->\")\n",
    "            for n in range(latents_final.shape[0]):\n",
    "                display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "            display(\"---------------\")\n",
    "\n",
    "        step = scheduler.step(noise_pred, t, latents, return_dict=True)#[0]\n",
    "        latents = step[\"prev_sample\"]\n",
    "\n",
    "\n",
    "clear_output(wait=True)\n",
    "display(f\"FINAL --->\")\n",
    "for n in range(latents_final.shape[0]):\n",
    "    display(f\"{n}    --->    \" + tokenizer.decode(vectors_to_indices(latents_final[n]), skip_special_tokens=True))\n",
    "display(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
